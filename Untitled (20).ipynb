{
 "cells": [
  {
   "cell_type": "raw",
   "id": "22eebc32-1892-47ca-a9c9-d03e35eb6f51",
   "metadata": {},
   "source": [
    "Q1\n",
    "Simple Linear Regression:\n",
    "\n",
    "Definition: Simple linear regression involves a single independent variable and a dependent variable.\n",
    "Equation: \n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+ϵ\n",
    "Example: Predicting a person's height based on their age. Here, height (y) is the dependent variable, and age (x) is the independent variable.\n",
    "\n",
    "Definition: Multiple linear regression involves two or more independent variables and a dependent variable.\n",
    "Equation: \n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "Example: Predicting a person's weight based on their height, age, and gender. Here, weight (y) is the dependent variable, and height (x1), age (x2), and gender (x3) are the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74864bee-c7ad-4647-8971-3fd272f78736",
   "metadata": {},
   "source": [
    "Q2\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables should be linear.\n",
    "Independence: The residuals (errors) should be independent.\n",
    "Homoscedasticity: The residuals should have constant variance at every level of the independent variables.\n",
    "Normality: The residuals should be normally distributed.\n",
    "No Multicollinearity: Independent variables should not be highly correlated with each other.\n",
    "\n",
    "How to Check These Assumptions:\n",
    "\n",
    "Linearity: Use scatter plots to visualize the relationship between each independent variable and the dependent variable.\n",
    "Independence: Check the Durbin-Watson statistic to assess the independence of residuals.\n",
    "Homoscedasticity: Plot the residuals against the predicted values and look for a pattern. Absence of a pattern suggests homoscedasticity.\n",
    "Normality: Use a Q-Q plot or the Shapiro-Wilk test to check if residuals are normally distributed.\n",
    "No Multicollinearity: Check the Variance Inflation Factor (VIF) values. VIF values greater than 10 indicate significant multicollinearity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c4488d-94d8-426b-8678-3657ba282abf",
   "metadata": {},
   "source": [
    "Q3\n",
    "Intercept (\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ): The expected value of the dependent variable when all independent variables are zero. It represents the baseline value.\n",
    "Slope (\n",
    "𝛽\n",
    "1\n",
    ",\n",
    "𝛽\n",
    "2\n",
    ",\n",
    "…\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,…): The expected change in the dependent variable for a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
    " \n",
    " Suppose you have a model predicting salary based on years of experience: \n",
    "Salary\n",
    "=\n",
    "30\n",
    ",\n",
    "000\n",
    "+\n",
    "2\n",
    ",\n",
    "000\n",
    "×\n",
    "Years of Experience\n",
    "Salary=30,000+2,000×Years of Experience.\n",
    "Intercept (\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ): $30,000, the baseline salary for someone with 0 years of experience.\n",
    "Slope (\n",
    "𝛽\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ): $2,000, the expected increase in salary for each additional year of experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f294d-0197-40be-961c-04deed3bed5e",
   "metadata": {},
   "source": [
    "Q4\n",
    "Definition: Gradient descent is an optimization algorithm used to minimize the cost function by iteratively moving towards the steepest descent, defined by the negative of the gradient.\n",
    "Usage in Machine Learning: It is used to find the optimal parameters (weights) that minimize the cost function in various machine learning models, including linear regression.\n",
    "How It Works:\n",
    "\n",
    "Start with initial guesses for the parameters.\n",
    "Calculate the gradient of the cost function with respect to each parameter.\n",
    "Update the parameters by moving in the direction opposite to the gradient by a step size (learning rate).\n",
    "Repeat until convergence (when the updates become negligible)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951bf73c-46d6-40f0-9489-9c325b4202d8",
   "metadata": {},
   "source": [
    "Q5\n",
    "Multiple Linear Regression:\n",
    "\n",
    "Equation: \n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "Definition: This model predicts the dependent variable based on multiple independent variables.\n",
    "Difference from Simple Linear Regression: Simple linear regression uses only one independent variable, while multiple linear regression uses two or more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c93ade-9760-4335-ab00-d7d04e53d715",
   "metadata": {},
   "source": [
    "Q6\n",
    "\n",
    "Definition: A situation where two or more independent variables in a regression model are highly correlated, making it difficult to distinguish their individual effects on the dependent variable.\n",
    "Detection:\n",
    "Variance Inflation Factor (VIF): VIF values above 10 suggest significant multicollinearity.\n",
    "Correlation Matrix: High correlation coefficients between independent variables indicate multicollinearity.\n",
    "Addressing Multicollinearity:\n",
    "Remove Highly Correlated Predictors: Exclude one of the correlated variables from the model.\n",
    "Principal Component Analysis (PCA): Transform the correlated variables into a set of uncorrelated components.\n",
    "Regularization Techniques: Apply techniques like Ridge Regression or Lasso Regression, which can handle multicollinearity by penalizing the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e13827-3768-4396-96f0-391674a12652",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
